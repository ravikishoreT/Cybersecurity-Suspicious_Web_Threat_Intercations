# Cybersecurity-Suspicious_Web_Threat_Interactions_p3
## Project Overview
#### Project Title : Cybersecurity: Suspicious Web Threat Interactions
#### Level : Intermediate
#### Tools : Visual Studio, Jupyter Notebook, Excel
#### Languages : Python
#### Databases : `CloudWatch_Traffic_Web_Attack`
This project is analyze web traffic data to identify suspicious patterns, develop features, visualize insights, and build a model for anomaly detection.

### About Data Set
This dataset contains web traffic records collected through AWS CloudWatch, aimed at detecting suspicious activities and potential attack attempts. The data were generated by monitoring traffic to a production web server, using various detection rules to identify anomalous patterns.

### Context
In today's cloud environments, cybersecurity is more crucial than ever. The ability to detect and respond to threats in real time can protect organizations from significant consequences. This dataset provides a view of web traffic that has been labeled as suspicious, offering a valuable resource for developers, data scientists, and security experts to enhance threat detection techniques.

### Data Set Content
#### Each entry in the dataset represents a stream of traffic to a web server, including the following columns:
#### bytes_in: Bytes received by the server.
#### bytes_out: Bytes sent from the server.
#### creation_time: Timestamp of when the record was created.
#### end_time: Timestamp of when the connection ended.
#### src_ip: Source IP address.src_ip_country_code: Country code of the source IP.
#### protocol: Protocol used in the connection.
#### response.code: HTTP response code.
#### dst_port: Destination port on the server.
#### dst_ip: Destination IP address.
#### rule_names: Name of the rule that identified the traffic as suspicious.
#### observation_name: Observations associated with the traffic.
#### source.meta: Metadata related to the source.
#### source.name: Name of the traffic source.
#### time: Timestamp of the detected event.detection_types: Type of detection applied.

### This dataset is ideal for:

#### ● Anomaly Detection: Developing models to detect unusual behaviors in web traffic.
#### ● Classification Models: Training models to automatically classify traffic as normal or suspicious.
#### ● Security Analysis: Conducting security analyses to understand the tactics, techniques, and procedures of attackers.

### 1. Import Modules Libraries
```python
import pandas as pd #Data Frames
import numpy as np  #Manipulating Numbers
import matplotlib.pyplot as plt #Visualisation
import seaborn as sns   #Visualisation
from sklearn.ensemble import IsolationForest 
sns.set_style('whitegrid') 
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import IsolationForest
```
### 2. Load & Explore The Data
```python
# Load the data
data = pd.read_csv(r"C:\Users\thamm\OneDrive\Documents\Internship\Projects\Cybersecurity-Suspicious Web Threat Interactions\CloudWatch_Traffic_Web_Attack.csv")

# Analyze the data
df = pd.DataFrame(data)
df

#Dimension of data
df.shape

#Basic information about the data
df.info()

#Rows
df.index

#Columns Names
df.columns

#Head of the data
df.head()

#Tail of the data
df.tail()

#Check for datatypes
df.dtypes
```
### 3. Data Cleaning & Preprocessing
The dataset contains 282 entries across 16 columns. There are no null values in any of the columns, which is good news for data integrity. However, let's proceed with the following data cleaning tasks: 
#### 1. Removing Duplicate Rows : Even though all entries appear non-null, there may still be duplicate entries that should be removed to prevent skewing our analysis.
### The data has been cleaned with the following steps implemented: 
#### 1. Duplicate Rows : No duplicate rows were found, so the dataset remains with 282 entries. 
#### 2. Data Types : The creation_time, end_time, and time columns have been successfully converted to datetime format, which is more appropriate for any operations involving time. 
#### 3. Text Data Standardization : The src_ip_country_code has been standardized to uppercase to ensure consistency across this field.

### 4. Handling Missing Values
```python
#Check for missing values for each column
data.isnull().sum()

#Remove duplicates rows
data.drop_duplicates()

# Fill or drop null values
data.dropna()

# Fill in 'bytes_in' with its median if missing.
df['bytes_in'] = df['bytes_in'].fillna(df['bytes_in'].median())
df['bytes_in']

# - Drop rows missing critical information like source or destination IP.
df.dropna(subset=['src_ip', 'dst_ip'], inplace=True)
df['src_ip']

df['dst_ip']
```
### 5. Convert to Date Time Format
```python
df['creation_time'] = pd.to_datetime(df['creation_time'])
df['end_time'] = pd.to_datetime(df['end_time'])
df['creation_time']

df['end_time']

df.head()
```

### 6. Exploratory Data Analysis(EDA)
Plot the distribution of bytes_in and bytes_out to explore traffic patterns.
```python
plt.figure(figsize=(12, 6))
sns.histplot(df['bytes_in'], bins=5, color='blue', kde=True,
label='Bytes In')
sns.histplot(df['bytes_out'], bins=5, color='red', kde=True,
label='Bytes Out')
plt.legend()
plt.title('Distribution of Bytes In and Bytes Out')
plt.show()
```
Count of protocols used
```python
plt.figure(figsize=(10,5))
sns.countplot(x='protocol', data=df, palette='viridis')
plt.title('Protocol Count', fontsize=14)
plt.xlabel('Protocol', fontsize=12)
plt.xticks(rotation=360)
plt.ylabel('Count', fontsize=12)
plt.show()
```

### 7.Feature Engineering
Extract useful features, like duration and average packet size, to aid in analysis.
```python
# Create a new feature: session_duration in seconds.
df['session_duration'] = (df['end_time'] - df['creation_time']).dt.total_seconds()
```
```python
# Create another feature: average packet size.
# Guard against division by zero by replacing 0 duration with NaN, then filling NaN with 0.
df['avg_packet_size'] = (df['bytes_in'] + df['bytes_out']) / df['session_duration'].replace(0, np.nan)
df['avg_packet_size'] = df['avg_packet_size'].fillna(0)
```
```python
df[['creation_time', 'end_time', 'session_duration', 'avg_packet_size']].head()
```

### 8. Data Visualization
Country-based interaction analysis based on source IP country codes.
```python
plt.figure(figsize=(12, 5))
sns.countplot(y='src_ip_country_code', data=df, order=df['src_ip_country_code'].value_counts().index)
plt.legend()
plt.title('Interaction Count by Source IP Country Code', fontsize=14)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Country Code', fontsize=12)
plt.show()
```
Suspicious Activities Based on Ports
```python
# we assume 'detection_types' indicates "Suspicious" traffic.

suspicious_df = df[df['detection_types'].str.contains('Suspicious', na=False)]
if not suspicious_df.empty:
    plt.figure(figsize=(12, 6))
    sns.countplot(x='dst_port', data=suspicious_df, palette='coolwarm')
    plt.title('Suspicious Activities Based on Destination Port')
    plt.xlabel('Destination Port')
    plt.xticks(rotation=45)
    plt.ylabel('Count')
    plt.show()
else:
    print("No records with 'Suspicious' in detection_types column found.")
```

### 8. Modeling: Anomaly Detection
This step uses Isolation Forest, a common technique for detecting anomalies.
```python
# Select features for anomaly detection.
features = df[['bytes_in', 'bytes_out', 'session_duration', 'avg_packet_size']].copy()
```
```python
# Fill any remaining NaN values in our features set for model stability.
features.fillna(features.mean(), inplace=True)
features
```
```python
# Initialize the Isolation Forest model.
model = IsolationForest(contamination=0.05, random_state=42)
model
```
```python
# Fit and predict anomalies
df['anomaly'] = model.fit_predict(features)
df['anomaly'] = df['anomaly'].apply(lambda x: 'Suspicious' if x == -1 else 'Normal')

df['anomaly'].value_counts()

suspicious_activities = df[df['anomaly'] == 'Suspicious']
suspicious_activities.head()
```

### 9. Visualizing the Anomalies
```python
# Visualize the relationship between bytes_in and bytes_out, highlighting anomalies.
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='bytes_in', y='bytes_out', hue='anomaly', palette={'Normal':'green', 'Suspicious':'red'})
plt.title('Anomalies in Bytes In vs Bytes Out', fontsize=14)
plt.xlabel('Bytes In', fontsize=12)
plt.ylabel('Bytes Out', fontsize=12)
plt.show()
```
### 10. Report finding
Based on the model output and visualizations, interpret the most frequent anomaly patterns, source IPs, and ports related to suspicious activities.
```python
df[df['anomaly'] == 'Suspicious'].head()
```

### 11. Conclusion
The conclusion for cybersecurity and suspicious web threat interactions emphasizes the continuous evolution of cyber threats and the need for proactive defense mechanisms. Cybersecurity is not a one-time solution but an ongoing process of monitoring, detecting, and mitigating risks posed by malicious web interactions.

